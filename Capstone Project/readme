# **Bitcoin Price Prediction Using Sentiment Analysis and Deep Learning**

This repository contains the code and documentation for a project focused on predicting Bitcoin prices using a combination of sentiment analysis from social media and various machine learning and deep learning models, integrated with multi-cryptocurrency data.

## **Project Overview**

The project aims to address the challenge of forecasting Bitcoin's volatile price by exploring the predictive power of historical price data, technical indicators, interdependencies with other major cryptocurrencies, and social media sentiment. It involves data collection, extensive preprocessing, feature engineering, comparative analysis of multiple models (ARIMA, Prophet, Random Forest, XGBoost, LSTM, GRU), and evaluation of their performance on unseen data.

## **Repository Structure**

The main components of this repository are:

- Capstone_Project.ipynb: The core Jupyter notebook containing all the code for data collection, preprocessing, exploratory data analysis, feature engineering, model implementation, training, evaluation, and visualization of results.
- Prediction of Bitcoin Prices.docx: The project report detailing the problem, literature review, methodology, results, analysis, and conclusions.
- Bitcoin_tweets_clean.csv (Expected): A CSV file containing the cleaned and preprocessed Bitcoin tweet data, which is loaded by the notebook. _Note: This file is assumed to be generated or included separately._
- Generated plots/figures (Expected): Various image files (.png or .jpg) generated by the notebook showing EDA visualizations, model loss curves, and actual vs. predicted price plots.

.  
├── Capstone_Project.ipynb  
├── Prediction of Bitcoin Prices.docx  
├── Bitcoin_tweets_clean.csv # Expected generated or included file  
└── plots/ # Expected directory for generated figures  
├── model_loss.png  
├── predictions.png  
└── ...  
<br/>

## **Dependencies**

To run the Jupyter notebook and reproduce the results, you need to install the following Python libraries. It is recommended to use a virtual environment.

pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras torch transformers yfinance pmdarima prophet xgboost lightgbm nltk tqdm  
<br/>

Specifically, the project uses:

- pandas for data manipulation and analysis.
- numpy for numerical operations.
- matplotlib and seaborn for data visualization.
- re for regular expressions (used in text cleaning).
- torch and transformers for the BERT-based sentiment analysis model.
- yfinance for fetching cryptocurrency price data.
- sklearn for machine learning utilities (splitting data, scaling, metrics, Random Forest, XGBoost).
- tensorflow and keras for building and training LSTM and GRU models.
- pmdarima for auto-fitting ARIMA models.
- prophet for the Prophet forecasting model.
- nltk for natural language processing tasks (text cleaning).
- tqdm for progress bars.

## **How to Reproduce Results**

Follow these steps to reproduce the project's results:

1. **Clone the repository:**  
    git clone &lt;repository_url&gt;  
    cd &lt;repository_name&gt;  

2. **Set up a virtual environment (recommended):**  
    python -m venv venv  
    source venv/bin/activate # On Windows use \`venv\\Scripts\\activate\`  

3. **Install dependencies:**  
    pip install -r requirements.txt # If a requirements.txt is provided  
    \# OR install manually using the list above  
    pip install pandas numpy matplotlib seaborn scikit-learn tensorflow keras torch transformers yfinance pmdarima prophet xgboost lightgbm nltk tqdm  

4. **Ensure data files are available:**
    - The notebook fetches historical cryptocurrency data using yfinance.
    - The raw tweet data used for cleaning and sampling can be downloaded from [Bitcoin Tweets Dataset on Kaggle](https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets/data). You would need to run the initial data collection and cleaning steps in the notebook (or a separate script) to generate the Bitcoin_tweets_clean.csv file used for sentiment analysis. The notebook code includes steps for cleaning and sampling tweet data.
5. **Run the Jupyter Notebook:**
    - Start a Jupyter Notebook or JupyterLab session in the repository directory:  
        jupyter notebook  
        \# or  
        jupyter lab  

    - Open the Capstone_Project.ipynb file.
    - Run all cells sequentially from top to bottom.

Running the notebook will execute the entire workflow: data loading/fetching, preprocessing, sentiment analysis, feature engineering, splitting data, training each model, evaluating their performance, and generating the plots and output metrics discussed in the report.

## **License**

\[Specify your project's license here, e.g., MIT, Apache 2.0, etc.\]

## **Contact**

\[Provide contact information or instructions for questions/feedback, e.g., GitHub issues, email address\]